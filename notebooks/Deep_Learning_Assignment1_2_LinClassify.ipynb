{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTJ_OJ1gtVLg"
      },
      "source": [
        "#Billedeklassificering med Linear classifiers\n",
        "\n",
        "I denne notebook arbejdes der med implementering og træning af lineære klassifikationsmodeller. Når I er færdige så skulle i gerne have fået indsigt i hvordan en parametrisk model er opbygget. Herunder hvordan loss/cost funktionen udregnes og hvordan gradienten af modellens parameter i forhold til dette bruges til at optimere modellen vha. af gradient decent.\n",
        "\n",
        "Som i assignment1-1 er det tilladt at bruge ChatGPT, CO-Pilot etc. til at løse opgaven."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXsZAVlmtucm"
      },
      "source": [
        "#Datasættet\n",
        "Ligesom for kNN så anvendes der i denne notebook data fra \"BloodMNIST\" datasættet som kommer fra [MedMNIST databasen](https://medmnist.com/)\n",
        "\n",
        "#Beskrivelse fra oprindelig kilde:\n",
        "The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set.\n",
        "\n",
        "Billederne er tilgængelige i forskellige opløsninger 28x28, 64x64, 128x128,og 224x224 med 3 kanaler (RGB). I denne notebook arbejder vi med billeder i opløsning 28x28x3. De 8 forskellige klasser er:\n",
        "\n",
        "['basophil', 'eosinophil', 'erythroblast', 'immature granulocytes', 'lymphocyte', 'monocyte', 'neutrophil', 'platelet']\n",
        "\n",
        "\n",
        "Angivet kategorisk med værdierne:\n",
        "\n",
        "[0, 1, 2, 3, 4, 5, 6, 7]\n",
        "\n",
        "Nedenfor downloades og importeres datasættet og opdeles i separate trænings- og valideringssæt (tuning) og nogle eksempler illustreres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lcu6k2plscKl"
      },
      "outputs": [],
      "source": [
        "%pip install medmnist\n",
        "import medmnist\n",
        "\n",
        "from medmnist import BloodMNIST\n",
        "trainDataset = BloodMNIST(split=\"train\", download=True,size=28)\n",
        "valDataset = BloodMNIST(split=\"val\", download=True,size=28)\n",
        "testDataset = BloodMNIST(split=\"test\", download=True,size=28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4tKnC6aswIG"
      },
      "outputs": [],
      "source": [
        "trainImages,trainLabels,trainInfo = trainDataset.__dict__['imgs'],trainDataset.__dict__['labels'],trainDataset.__dict__['info']['label']\n",
        "\n",
        "\n",
        "print('Træningsdata:')\n",
        "print(f'Billeder: {trainImages.shape}, Labels: {trainLabels.shape}')\n",
        "\n",
        "\n",
        "valImages,valLabels = valDataset.__dict__['imgs'],valDataset.__dict__['labels']\n",
        "print('Valideringsdata:')\n",
        "print(f'Billeder: {valImages.shape}, Labels: {valLabels.shape}')\n",
        "\n",
        "testImages,testLabels = testDataset.__dict__['imgs'],testDataset.__dict__['labels']\n",
        "print('Testdata:')\n",
        "print(f'Billeder: {testImages.shape}, Labels: {testLabels.shape}')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "print('\\n')\n",
        "print('plot nogle eksempler:')\n",
        "\n",
        "random.seed(42)\n",
        "fig, axes = plt.subplots(5, len(trainInfo), figsize=(15, 5))\n",
        "\n",
        "for class_,name in trainInfo.items():\n",
        "    print(f'klasse: {class_}, klassenavn: {name}. Antal træning samples: {len(trainLabels[trainLabels==int(class_)])}')\n",
        "    # Get indices of all images belonging to class i\n",
        "    class_indices = [idx for idx, label in enumerate(trainLabels) if int(class_) == label]\n",
        "    # Randomly select 5 indices\n",
        "    selected_indices = random.sample(class_indices, 5)\n",
        "    for j, idx in enumerate(selected_indices):\n",
        "        image, label = trainImages[idx],trainLabels[idx]\n",
        "        axes[j, int(class_)].imshow(image, cmap='gray')\n",
        "        axes[j, int(class_)].axis('off')\n",
        "        if j == 0:\n",
        "            axes[j, int(class_)].set_title(f'{name[:5]}: {class_}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3gyp0vts1di"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#set random seed for reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "#Shuffle the data\n",
        "randomize = np.arange(trainImages.shape[0])\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "X_train = trainImages[randomize]\n",
        "y_train = trainLabels[randomize].flatten()\n",
        "\n",
        "randomizeVal = np.arange(valImages.shape[0])\n",
        "X_val = valImages[randomizeVal]\n",
        "y_val = valLabels[randomizeVal].flatten()\n",
        "\n",
        "\n",
        "# Subsample the data for more efficient code execution in this exercise\n",
        "num_training = 5000\n",
        "mask = list(range(num_training))\n",
        "\n",
        "X_train = X_train[mask]\n",
        "y_train = y_train[mask]\n",
        "\n",
        "print('first 10 examples in train: ',y_train[:10])\n",
        "\n",
        "num_val = 500\n",
        "mask = list(range(num_val))\n",
        "X_val = X_val[mask]\n",
        "y_val = y_val[mask]\n",
        "\n",
        "print('first 10 examples in val: ',y_val[:10])\n",
        "\n",
        "# Reshape the image data into rows for effecient distance calculation\n",
        "#(vi tager billedet med dimensioner 28x28x3 og strækker det ud til en vektor med længden 28*28*3 = 2352)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "print(f'New train shape: {X_train.shape}')\n",
        "print(f'New val shape: {X_val.shape}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z30JswZduSLL"
      },
      "outputs": [],
      "source": [
        "# Preprocessing: subtract the mean image\n",
        "# first: compute the image mean based on the training data\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "print(mean_image[:10]) # print a few of the elements\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(mean_image.reshape((28,28,3)).astype('uint8')) # visualize the mean image\n",
        "plt.show()\n",
        "\n",
        "# second: subtract the mean image from train and test data\n",
        "X_train = X_train.astype(np.float32)-mean_image\n",
        "X_val = X_val.astype(np.float32)-mean_image\n",
        "\n",
        "print(X_train.min(),X_train.max())\n",
        "\n",
        "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
        "# only has to worry about optimizing a single weight matrix W.\n",
        "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "\n",
        "print(X_train.shape, X_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Standadizing data\n",
        "You could consider standardizing you data, e.g divide by 255 to get it on a scale from [-1,1]. This may help to avoid exploding gradients since the current values of the input data are quite large."
      ],
      "metadata": {
        "id": "0WwV1vOlAXQZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CEk6Lwb3KnJ"
      },
      "source": [
        "#Implementation of SVM/Hinge-loss\n",
        "Below, implement a function to for calculating the hinge loss based on an image/set of images and associated class/label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-69Y59mc4hEL"
      },
      "outputs": [],
      "source": [
        "#################################################\n",
        "#generate a random SVM weight matrix of small\n",
        "#numbers corresponding to image size (+ bias dimension)\n",
        "#depth of num classes\n",
        "##################################################\n",
        "image_size = 28*28*3+1\n",
        "num_classes = 8\n",
        "\n",
        "W = np.random.randn(image_size, num_classes) * 0.0001\n",
        "\n",
        "###################################################\n",
        "#The function should accept The W weight-/parameter\n",
        "#matrix along with an image/batch of images as input\n",
        "#and return the loss for the\n",
        "#image/batch based on the claculated scores\n",
        "#The l2 regularization loss should also be included\n",
        "#in the function's claculation.\n",
        "#In order to check the implementation, try to claculate\n",
        "#the loss but with, and without regularization.\n",
        "#For the hinge/svm loss, the expected loss from random\n",
        "#initialized wieghts ~= k-1 where k is number of classes\n",
        "####################################################\n",
        "\n",
        "def svm_loss(W,X,y,reg=0):\n",
        "  pass\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XVORYZl_7r9"
      },
      "source": [
        "#Test the loss using the validation set\n",
        "\n",
        "E.g.:\n",
        "\n",
        "    loss = svm_loss(W,X,y,reg):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDHRYh6Y_R7W"
      },
      "outputs": [],
      "source": [
        "#Test the loss with and without regularization using the validation set\n",
        "#With regularization the loss should increase.\n",
        "#test with large value. e.g. 1e3 to make sure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH8545GJFmbZ"
      },
      "source": [
        "#Compute the gradients of the weights with respect to the loss\n",
        "In order to optimize the classifier we need to compute gradients for the weight matrix so the loss can be minimized. It is would be a good idea to include the gradient calculation in the loss function defined above such that scores, loss and gradient can be calculated at the same time using the function.\n",
        "\n",
        "E.g.:\n",
        "\n",
        "    loss,grad = svm_loss(W,X,y,reg)\n",
        "\n",
        "\n",
        "The gradients are a matrix with the same shape a W. The regularization should also affect the gradient so make sure this is included in the computation\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgBWOQXYLaIu"
      },
      "outputs": [],
      "source": [
        "###################################################\n",
        "#Once the implementation is complete try to print the\n",
        "#shape of the gradient matrix as well as the first row\n",
        "#It should return something similar to this:\n",
        "# [-3.0962016  4.9891232 -4.0875872  0.8926112  1.0465696 -1.5291232\n",
        "#  3.9497376 -2.1651296]\n",
        "#####################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N555rsqNbu4"
      },
      "source": [
        "#Implementation of Softmax-loss\n",
        "Below, repeat the steps from above but for the softmax loss/gradient instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeGxLeYuObAk"
      },
      "outputs": [],
      "source": [
        "def softmax_loss(W,X,y,reg=0):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCtvrXrGPN0g"
      },
      "source": [
        "Test the loss and gradient calculations\n",
        "\n",
        "E.g.:\n",
        "\n",
        "    loss,grad = softmax_loss(W,X,y,reg)\n",
        "\n",
        "The loss with an randomly initialized weight matrix should be ~ -ln(1/k) where k=number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2a3bfwARYcK"
      },
      "outputs": [],
      "source": [
        "###################################################\n",
        "#Once the implementation is complete try to print the\n",
        "#loss, shape of the gradient matrix as well as the first row\n",
        "#The gradient claculation should return something similar to this:\n",
        "# [-0.36533844  0.69120216 -0.47791155  0.10853622  0.05629067 -0.23380291\n",
        "#  0.50159745 -0.2805736 ]\n",
        "#####################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpV3-uf9xiAo"
      },
      "source": [
        "#Implementation of linear classifier\n",
        "\n",
        "The implementation of the linear classifier should be constructed as a class that initializes a weight matrix based on the shape of the input and number of classes and type of loss computed (SVM/Softmax). It should have methods for training (updating the weights) and inference (predicting classes from scores).\n",
        "\n",
        "Note!\n",
        "\n",
        "The training function should propably have a \"batch_size\" parameter such that training data is iterated in batches so as to not run out of memory. Use np.array_split to divide data into n batches of \"batch_size\".\n",
        "\n",
        "psuedo code example:\n",
        "\n",
        "\n",
        "    class LinearClassifier:\n",
        "      def __init__(self, input_dim, num_classes, loss_type='softmax'):\n",
        "\n",
        "          self.W = np.random.randn(input_dim, num_classes) * 0.01\n",
        "          self.loss_type = loss_type\n",
        "\n",
        "          method train()\n",
        "\n",
        "          method softmax_loss()\n",
        "\n",
        "          method svm_loss()\n",
        "\n",
        "          method predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRNZvyUMxrC8"
      },
      "outputs": [],
      "source": [
        "\n",
        "##################################\n",
        "#Implement linear classifier class\n",
        "##################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the classifier once on the training set\n",
        "It might be a good idea to have the training function of you model return a \"history\" or similar object holding the values of the loss at each m steps of the training. Afterwards the history object (for instance a list) can be plottet to visualize how training progressed.\n",
        "\n",
        "Calling the training function would thus look something like this:\n",
        "\n",
        "    classifier = LinearClassifier(input_dim=28*28*3+1,num_classes=8, loss_type='svm')\n",
        "\n",
        "    history = classifier.train(params)\n",
        "\n",
        "    plot(history)\n",
        "\n",
        "    #make predictions on the validation data\n",
        "    pred = classifier.predict(X)\n",
        "\n",
        "    #calculate the accuracy"
      ],
      "metadata": {
        "id": "35Zx3O3KdNDQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rth4zEt9T3xX"
      },
      "outputs": [],
      "source": [
        "##########################################\n",
        "#Train the classifier and plot the results\n",
        "##########################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93J9Ai3MyouZ"
      },
      "source": [
        "#Optimize SVM classifier\n",
        "We start by optimizing the the classifier with the svm loss function.\n",
        "\n",
        "Using the training and validation sets run training loops with different setting for learning rate and regularization strength. For each setting, iterate over the training set n times updating the weight matrix based on the gradients and evaluate performance on the validation set every m iterations. Save the best classifier object in a variable \"best_svm\".\n",
        "\n",
        "\n",
        "E.g.\n",
        "\n",
        "    best_svm = None\n",
        "    best_accuracy = 0\n",
        "\n",
        "    # These values are only given as examples. In your own loop\n",
        "    # you should test more in order to optimize the classifier\n",
        "\n",
        "    learning_rates = [1e-2,1e-4,1e-6]\n",
        "    regularization_strenghts = [1,1e-2,1e-1,1e2]\n",
        "\n",
        "    num_iters = 1000\n",
        "    batch_size = 200\n",
        "\n",
        "    for lr in learning_rates:\n",
        "      for reg in regularizations_strenghts:\n",
        "\n",
        "        svm = LinearClassifier(input_dim=28*28*3+1,num_classes=8,loss_type='svm')\n",
        "\n",
        "        svm.train(params)\n",
        "\n",
        "        pred = svm.predict(X)\n",
        "\n",
        "        #calculate accuracy\n",
        "\n",
        "        \n",
        "        if accuracy > best_accuracy:\n",
        "          best_svm = svm\n",
        "          best_accuracy = accuracy\n",
        "\n",
        "\n",
        "! It may take a long time for the model to converge. Therefor, the hyperparameter search in this notebook could be used to identify a reasonable set of hyperparameters and then the model should be trained for longer using these. Using the full training dataset to maximize performance\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "#Put optimization code here\n",
        "###########################\n",
        "\n"
      ],
      "metadata": {
        "id": "sAxrpuNME-yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I could be a good idea to record as much informartion as possible from the optimization. E.g. loss as function of hyperparameters. This can be used for visualizing results of the tuning and included in the assignment 1 report."
      ],
      "metadata": {
        "id": "wADOw9TyPlqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#visualize the weights\n",
        "Depending on how you implemented the classifier it is possible to visualize the learned weights. I the classifier is well optimized you may notice the sort of class templates encoded in the weight matrix.\n",
        "\n"
      ],
      "metadata": {
        "id": "4I22I3g1NPqz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1qf37E7mV5L"
      },
      "outputs": [],
      "source": [
        " #Visualization of learned weights. This may or may not work depending on how your classifier is implemented.\n",
        "w = best_svm.W[:-1,:] # strip out the bias\n",
        "w = w.reshape(28, 28, 3, 8)\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "classes = ['0', '1', '2', '3', '4', '5', '6', '7']\n",
        "for i in range(8):\n",
        "    plt.subplot(1, 8, i + 1)\n",
        "\n",
        "    # Rescale the weights to be between 0 and 255\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can compare the learned weights to a set of randomly initialized weights"
      ],
      "metadata": {
        "id": "4vDUEmJdUSYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare the trained weights to a set of random initialized weights\n",
        "w = np.random.randn(28*28*3+1, num_classes) * 0.001\n",
        "w = w[:-1,:] # strip out the bias\n",
        "w = w.reshape(28, 28, 3, 8)\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "classes = ['0', '1', '2', '3', '4', '5', '6', '7']\n",
        "for i in range(8):\n",
        "    plt.subplot(1, 8, i + 1)\n",
        "\n",
        "    # Rescale the weights to be between 0 and 255\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])"
      ],
      "metadata": {
        "id": "jYt7VXbiT8za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimize softmax classifier\n",
        "Repeat the steps above but using the linear classifier with softmax loss. Report results for both in assignment 1\n",
        "\n",
        "In the hand-in for assignment 1 you should be able to explain the implementation of the loss and gradients for the two types of linear classifiers. Therefore it might be good to include relevant code snippets.\n",
        "\n",
        "The Hand in should include the tuning results/graphs (train and validation to discuss overfitting potential) and test set results think about metrics that make sense to calculate and include. E.g confusion matrix."
      ],
      "metadata": {
        "id": "lHZ3SmtTcEIr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}