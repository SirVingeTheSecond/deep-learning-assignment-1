{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Billedeklassificering med Neurale Netværk\n",
        "\n",
        "I denne notebook arbejdes der med implementering og træning af et simpelt neuralt netværk. Når I er færdige så skulle i gerne have fået indsigt i hvordan denne type af modeller, hvor linære transformationer arrangeret i lag med mellemliggende non-linære \"activation functions\" kan bruges i kontruktion af mere komplekse klassificeringsmodeller, og hvilke hyperparametere der kan tunes under træning af disse. Herunder hvordan loss/cost funktionen udregnes og hvordan gradienten af loss i henhold til modellens parameter bruges til at optimere modellen vha. af gradient decent.\n",
        "\n"
      ],
      "metadata": {
        "id": "pS_XYfL-vG8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Datasættet\n",
        "Ligesom for kNN og liniære modeller, så anvendes der i denne notebook data fra \"BloodMNIST\" datasættet som kommer fra [MedMNIST databasen](https://medmnist.com/)\n",
        "\n",
        "#Beskrivelse fra oprindelig kilde:\n",
        "The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes. We split the source dataset with a ratio of 7:1:2 into training, validation and test set.\n",
        "\n",
        "Billederne er tilgængelige i forskellige opløsninger 28x28, 64x64, 128x128,og 224x224 med 3 kanaler (RGB). I denne notebook arbejder vi med billeder i opløsning 28x28x3. De 8 forskellige klasser er:\n",
        "\n",
        "['basophil', 'eosinophil', 'erythroblast', 'immature granulocytes', 'lymphocyte', 'monocyte', 'neutrophil', 'platelet']\n",
        "\n",
        "\n",
        "Angivet kategorisk med værdierne:\n",
        "\n",
        "[0, 1, 2, 3, 4, 5, 6, 7]\n",
        "\n",
        "Nedenfor downloades og importeres datasættet og opdeles i separate trænings- og valideringssæt (tuning) og nogle eksempler illustreres. (samme procedure som for liniære modeller)"
      ],
      "metadata": {
        "id": "FBT_aEqpSp76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install medmnist\n",
        "import medmnist\n",
        "\n",
        "from medmnist import BloodMNIST\n",
        "trainDataset = BloodMNIST(split=\"train\", download=True,size=28)\n",
        "valDataset = BloodMNIST(split=\"val\", download=True,size=28)\n",
        "testDataset = BloodMNIST(split=\"test\", download=True,size=28)"
      ],
      "metadata": {
        "id": "qsHIShgxR1vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainImages,trainLabels,trainInfo = trainDataset.__dict__['imgs'],trainDataset.__dict__['labels'],trainDataset.__dict__['info']['label']\n",
        "\n",
        "\n",
        "print('Træningsdata:')\n",
        "print(f'Billeder: {trainImages.shape}, Labels: {trainLabels.shape}')\n",
        "\n",
        "\n",
        "valImages,valLabels = valDataset.__dict__['imgs'],valDataset.__dict__['labels']\n",
        "print('Valideringsdata:')\n",
        "print(f'Billeder: {valImages.shape}, Labels: {valLabels.shape}')\n",
        "\n",
        "testImages,testLabels = testDataset.__dict__['imgs'],testDataset.__dict__['labels']\n",
        "print('Testdata:')\n",
        "print(f'Billeder: {testImages.shape}, Labels: {testLabels.shape}')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "print('\\n')\n",
        "print('plot nogle eksempler:')\n",
        "\n",
        "random.seed(42)\n",
        "fig, axes = plt.subplots(5, len(trainInfo), figsize=(15, 5))\n",
        "\n",
        "for class_,name in trainInfo.items():\n",
        "    print(f'klasse: {class_}, klassenavn: {name}. Antal træning samples: {len(trainLabels[trainLabels==int(class_)])}')\n",
        "    # Get indices of all images belonging to class i\n",
        "    class_indices = [idx for idx, label in enumerate(trainLabels) if int(class_) == label]\n",
        "    # Randomly select 5 indices\n",
        "    selected_indices = random.sample(class_indices, 5)\n",
        "    for j, idx in enumerate(selected_indices):\n",
        "        image, label = trainImages[idx],trainLabels[idx]\n",
        "        axes[j, int(class_)].imshow(image, cmap='gray')\n",
        "        axes[j, int(class_)].axis('off')\n",
        "        if j == 0:\n",
        "            axes[j, int(class_)].set_title(f'{name[:5]}: {class_}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D3b_8ovnSzEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#set random seed for reproducibility\n",
        "np.random.seed(0)\n",
        "\n",
        "#Shuffle the data\n",
        "randomize = np.arange(trainImages.shape[0])\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "X_train = trainImages[randomize]\n",
        "y_train = trainLabels[randomize].flatten()\n",
        "\n",
        "randomizeVal = np.arange(valImages.shape[0])\n",
        "X_val = valImages[randomizeVal]\n",
        "y_val = valLabels[randomizeVal].flatten()\n",
        "\n",
        "\n",
        "# Subsample the data for more efficient code execution in this exercise\n",
        "num_training = 5000\n",
        "mask = list(range(num_training))\n",
        "\n",
        "X_train = X_train[mask]\n",
        "y_train = y_train[mask]\n",
        "\n",
        "print('first 10 examples in train: ',y_train[:10])\n",
        "\n",
        "num_val = 500\n",
        "mask = list(range(num_val))\n",
        "X_val = X_val[mask]\n",
        "y_val = y_val[mask]\n",
        "\n",
        "print('first 10 examples in val: ',y_val[:10])\n",
        "\n",
        "# Reshape the image data into rows for effecient distance calculation\n",
        "#(vi tager billedet med dimensioner 28x28x3 og strækker det ud til en vektor med længden 28*28*3 = 2352)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "print(f'New train shape: {X_train.shape}')\n",
        "print(f'New val shape: {X_val.shape}')\n"
      ],
      "metadata": {
        "id": "2aelo5gYTBQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing: subtract the mean image\n",
        "# first: compute the image mean based on the training data\n",
        "mean_image = np.mean(X_train, axis=0)\n",
        "print(mean_image[:10]) # print a few of the elements\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.imshow(mean_image.reshape((28,28,3)).astype('uint8')) # visualize the mean image\n",
        "plt.show()\n",
        "\n",
        "# second: subtract the mean image from train and test data\n",
        "X_train = X_train.astype(np.float32)-mean_image\n",
        "X_val = X_val.astype(np.float32)-mean_image\n",
        "\n",
        "print(X_train.min(),X_train.max())\n",
        "\n",
        "print(X_train.shape, X_val.shape)"
      ],
      "metadata": {
        "id": "Y3R2gQVyTikk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Standadizing data\n",
        "We standardize the data, i.e. divide by 255 to get it on a scale from [-1,1]. This may help to avoid exploding gradients since the current values of the input data are quite large.\n",
        "\n"
      ],
      "metadata": {
        "id": "oNx_zqQDTnwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train /= 255.\n",
        "X_val /= 255."
      ],
      "metadata": {
        "id": "kMK7h_xBC7WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation of simple, fully connected neural network\n",
        "One way could be to construct the model using a modular approach. I.e. define the different building blocks of the model seperately such that they can be put together arbitrarily at a later point. E.g. for constructing networks with more layers.\n",
        "\n",
        "So for instance, each layer in a neural network performs two operations. A forward operation/\"pass\", where an output is computed based the input to the layer and the layers internal parameters. The output is then passed to the subsequent layer along with a cache object storing data needed for the backward pass.\n",
        "\n",
        "In the backwards operation, the layer recieves the backwards/upstream gradient with information on how the output affected the loss computation at the end of the network And using the cache object the gradient with respect the the layers inputs (dx,dW) are calculated and the gradients dW can then be used to optimize the parameters.\n",
        "\n",
        "Below, we have and implementation of the simple neural network."
      ],
      "metadata": {
        "id": "MbzgeLTtT4MU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class FullyConnectedNN:\n",
        "    def __init__(self, layers, reg_strength=0.01, loss='softmax', seed=42):\n",
        "        \"\"\"\n",
        "        layers: List where each element represents the number of nodes in that layer.\n",
        "        reg_strength: L2 regularization strength\n",
        "        loss: 'softmax' or 'hinge'\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "        self.layers = layers\n",
        "        self.reg_strength = reg_strength\n",
        "        self.loss_type = loss\n",
        "        self.params = self._initialize_weights(layers)\n",
        "\n",
        "    def _initialize_weights(self, layers):\n",
        "        \"\"\"\n",
        "        Initialize weights and biases for each layer\n",
        "        \"\"\"\n",
        "        params = {}\n",
        "        for i in range(1, len(layers)):\n",
        "            params['W' + str(i)] = np.random.randn(layers[i-1], layers[i]) * 0.01\n",
        "            params['b' + str(i)] = np.zeros((1, layers[i]))\n",
        "        return params\n",
        "\n",
        "    def relu(self, Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    def relu_derivative(self, Z):\n",
        "        return Z > 0\n",
        "\n",
        "    def softmax(self, Z):\n",
        "        exp_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "        return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
        "\n",
        "    def softmax_loss(self, A, y):\n",
        "        m = y.shape[0]\n",
        "        p = self.softmax(A)\n",
        "        log_likelihood = -np.log(p[range(m), y])\n",
        "        loss = np.sum(log_likelihood) / m\n",
        "        return loss\n",
        "\n",
        "    def hinge_loss(self, A, y):\n",
        "        m = y.shape[0]\n",
        "        correct_class_scores = A[range(m), y].reshape(-1, 1)\n",
        "        margins = np.maximum(0, A - correct_class_scores + 1)\n",
        "        margins[range(m), y] = 0\n",
        "        loss = np.sum(margins) / m\n",
        "        return loss\n",
        "\n",
        "    def compute_loss(self, A, y):\n",
        "        if self.loss_type == 'softmax':\n",
        "            return self.softmax_loss(A, y) + self._l2_regularization()\n",
        "        elif self.loss_type == 'hinge':\n",
        "            return self.hinge_loss(A, y) + self._l2_regularization()\n",
        "\n",
        "    def _l2_regularization(self):\n",
        "        reg_loss = 0\n",
        "        for i in range(1, len(self.layers)):\n",
        "            reg_loss += np.sum(np.square(self.params['W' + str(i)]))\n",
        "        return self.reg_strength * reg_loss / 2\n",
        "\n",
        "    def forward(self, X):\n",
        "        cache = {'A0': X}\n",
        "        A = X\n",
        "        for i in range(1, len(self.layers)):\n",
        "            W, b = self.params['W' + str(i)], self.params['b' + str(i)]\n",
        "            Z = np.dot(A, W) + b\n",
        "            if i != len(self.layers) - 1:\n",
        "                A = self.relu(Z)\n",
        "            else:\n",
        "                A = Z  # No activation in the output layer (raw scores for loss)\n",
        "            cache['Z' + str(i)] = Z\n",
        "            cache['A' + str(i)] = A\n",
        "        return A, cache\n",
        "\n",
        "    def backward(self, cache, y):\n",
        "        grads = {}\n",
        "        m = y.shape[0]\n",
        "        A_last = cache['A' + str(len(self.layers) - 1)]\n",
        "        if self.loss_type == 'softmax':\n",
        "            dA = self.softmax(A_last)\n",
        "            dA[range(m), y] -= 1\n",
        "            dA /= m\n",
        "        elif self.loss_type == 'hinge':\n",
        "            margins = (A_last - A_last[range(m), y].reshape(-1, 1) + 1) > 0\n",
        "            margins[range(m), y] = 0\n",
        "            dA = np.where(margins, 1, 0)\n",
        "            dA /= m\n",
        "\n",
        "        for i in reversed(range(1, len(self.layers))):\n",
        "            dZ = dA\n",
        "            A_prev = cache['A' + str(i - 1)]\n",
        "            grads['W' + str(i)] = np.dot(A_prev.T, dZ) + self.reg_strength * self.params['W' + str(i)]\n",
        "            grads['b' + str(i)] = np.sum(dZ, axis=0, keepdims=True)\n",
        "            if i > 1:\n",
        "                dA = np.dot(dZ, self.params['W' + str(i)].T) * self.relu_derivative(cache['Z' + str(i - 1)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def update_params(self, grads, learning_rate, v=None, beta1=0.9, beta2=0.999, t=1, optimizer='sgd'):\n",
        "        \"\"\"\n",
        "        Updates parameters with chosen optimization method.\n",
        "        If optimizer is 'momentum' or 'adam', it requires v for velocity and also t for time step in adam.\n",
        "        \"\"\"\n",
        "        for i in range(1, len(self.layers)):\n",
        "            if optimizer == 'sgd':\n",
        "                self.params['W' + str(i)] -= learning_rate * grads['W' + str(i)]\n",
        "                self.params['b' + str(i)] -= learning_rate * grads['b' + str(i)]\n",
        "            elif optimizer == 'momentum':\n",
        "                v['dW' + str(i)] = beta1 * v['dW' + str(i)] + (1 - beta1) * grads['W' + str(i)]\n",
        "                v['db' + str(i)] = beta1 * v['db' + str(i)] + (1 - beta1) * grads['b' + str(i)]\n",
        "                self.params['W' + str(i)] -= learning_rate * v['dW' + str(i)]\n",
        "                self.params['b' + str(i)] -= learning_rate * v['db' + str(i)]\n",
        "            elif optimizer == 'adam':\n",
        "                v['mW' + str(i)] = beta1 * v['mW' + str(i)] + (1 - beta1) * grads['W' + str(i)]\n",
        "                v['vW' + str(i)] = beta2 * v['vW' + str(i)] + (1 - beta2) * np.square(grads['W' + str(i)])\n",
        "                mW_hat = v['mW' + str(i)] / (1 - beta1**t)\n",
        "                vW_hat = v['vW' + str(i)] / (1 - beta2**t)\n",
        "                self.params['W' + str(i)] -= learning_rate * mW_hat / (np.sqrt(vW_hat) + 1e-8)\n",
        "\n",
        "    def fit(self, X, y, epochs=100, batch_size=64, learning_rate=0.01, optimizer='sgd'):\n",
        "        \"\"\"\n",
        "        Trains the model using the chosen optimizer.\n",
        "        \"\"\"\n",
        "        v = None\n",
        "        if optimizer in ['momentum', 'adam']:\n",
        "            v = {}\n",
        "            for i in range(1, len(self.layers)):\n",
        "                v['dW' + str(i)] = np.zeros_like(self.params['W' + str(i)])\n",
        "                v['db' + str(i)] = np.zeros_like(self.params['b' + str(i)])\n",
        "                if optimizer == 'adam':\n",
        "                    v['mW' + str(i)] = np.zeros_like(self.params['W' + str(i)])\n",
        "                    v['vW' + str(i)] = np.zeros_like(self.params['W' + str(i)])\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            permutation = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[permutation]\n",
        "            y_shuffled = y[permutation]\n",
        "\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                X_batch = X_shuffled[i:i + batch_size]\n",
        "                y_batch = y_shuffled[i:i + batch_size]\n",
        "\n",
        "                A_last, cache = self.forward(X_batch)\n",
        "                loss = self.compute_loss(A_last, y_batch)\n",
        "                grads = self.backward(cache, y_batch)\n",
        "                self.update_params(grads, learning_rate, v=v, optimizer=optimizer)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        A_last, _ = self.forward(X)\n",
        "        if self.loss_type == 'softmax':\n",
        "            return np.argmax(A_last, axis=1)\n",
        "        elif self.loss_type == 'hinge':\n",
        "            return np.argmax(A_last, axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "HFSHVmX-NpcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "# To-lags neuralt netværk med input lag med antal neuroner svarende til input\n",
        "# størrelsen, ét \"hidden\" lag med 200 neuroner, og et output lag\n",
        "# med antal neuroner = antallet af klasser i datasættet.\n",
        "\n",
        "nn = FullyConnectedNN(layers=[X_train.shape[1], 500, 8], loss='softmax')\n",
        "\n",
        "# Train the neural network on the subsampled training data for 200 epochs\n",
        "# using stochastic gradient decent\n",
        "nn.fit(X_train, y_train,learning_rate=0.01, epochs=200,optimizer='sgd')"
      ],
      "metadata": {
        "id": "VJfUoIgxu9Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classify training data and val data using the trained model\n",
        "preds = nn.predict(X_train)\n",
        "\n",
        "print(f'Training accuracy={np.mean(preds==y_train)}')\n",
        "\n",
        "\n",
        "val_preds = nn.predict(X_val)\n",
        "\n",
        "print(f'Validation accuracy={np.mean(val_preds==y_val)}')"
      ],
      "metadata": {
        "id": "0UthN0zMRQ_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimize the model\n",
        "Similar to assignment1-1 and assignment1-2 you should now optimize the model. In this notebook we have more hyperparameteres compared to earlier. We can tune the number of layers in the network, number of nodes/neurons in each layer, learning rate, regularization strength, optimizer, loss function and possibly batch size. You should now write you own optimization code. Train the model on the training set and tune hyperparameters on the validation set to find the optimal configuration which maximizes validation accuracy/minimizes validation loss. Record loss/accuracy history on the training and validation data for later visualization.\n",
        "\n",
        "Once the model has been optimized, test performance on the test set."
      ],
      "metadata": {
        "id": "brqaAvlYYKI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "######### your optimization code goes here #############################\n",
        "########################################################################"
      ],
      "metadata": {
        "id": "DZyCaC_PYQj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualize results"
      ],
      "metadata": {
        "id": "-ikzIkolZeaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "######### visualize hyperparameter tuning history/results ##############\n",
        "########################################################################"
      ],
      "metadata": {
        "id": "6fPzImuuY1TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate performance"
      ],
      "metadata": {
        "id": "qsvFspuOZhd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "######## evaluate on the test data. Consider including    #############\n",
        "######## relevant metrics such as confusin matrices etc.  #############\n",
        "#######################################################################"
      ],
      "metadata": {
        "id": "cA1G5t79ZE0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment report\n",
        "In the assignment report you should report the results the same way as for assignment1-1 and assignment1-2. I.e. outcome of the hyperparamter search and test set results and metrics.\n",
        "\n",
        "Since the implementation of the network class has been written. You should make sure to fully understand it and explain relevant implementation details (with code snippets) in the report."
      ],
      "metadata": {
        "id": "XK2oxzVpaH6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualize learned weights\n",
        "Like in assignment1-2 we can visualize the learned weights/parameters of the model. If the model has been well optimized you should see some patterns resembling the input data."
      ],
      "metadata": {
        "id": "afSKSEmLZksU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import ceil, sqrt\n",
        "def visualize_grid(Xs, ubound=255.0, padding=1):\n",
        "    \"\"\"\n",
        "    Reshape a 4D tensor of image data to a grid for easy visualization.\n",
        "\n",
        "    Inputs:\n",
        "    - Xs: Data of shape (N, H, W, C)\n",
        "    - ubound: Output grid will have values scaled to the range [0, ubound]\n",
        "    - padding: The number of blank pixels between elements of the grid\n",
        "    \"\"\"\n",
        "    (N, H, W, C) = Xs.shape\n",
        "    grid_size = int(ceil(sqrt(N)))\n",
        "    grid_height = H * grid_size + padding * (grid_size - 1)\n",
        "    grid_width = W * grid_size + padding * (grid_size - 1)\n",
        "    grid = np.zeros((grid_height, grid_width, C))\n",
        "    next_idx = 0\n",
        "    y0, y1 = 0, H\n",
        "    for y in range(grid_size):\n",
        "        x0, x1 = 0, W\n",
        "        for x in range(grid_size):\n",
        "            if next_idx < N:\n",
        "                img = Xs[next_idx]\n",
        "                low, high = np.min(img), np.max(img)\n",
        "                grid[y0:y1, x0:x1] = ubound * (img - low) / (high - low)\n",
        "                # grid[y0:y1, x0:x1] = Xs[next_idx]\n",
        "                next_idx += 1\n",
        "            x0 += W + padding\n",
        "            x1 += W + padding\n",
        "        y0 += H + padding\n",
        "        y1 += H + padding\n",
        "    # grid_max = np.max(grid)\n",
        "    # grid_min = np.min(grid)\n",
        "    # grid = ubound * (grid - grid_min) / (grid_max - grid_min)\n",
        "    return grid"
      ],
      "metadata": {
        "id": "EvqwAr5OS19j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_net_weights(net):\n",
        "    W1 = net.params['W1']\n",
        "    W1 = W1.reshape(3, 28, 28, -1).transpose(3, 1, 2, 0)\n",
        "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
        "    plt.gca().axis('off')\n",
        "    plt.show()\n",
        "\n",
        "show_net_weights(nn)"
      ],
      "metadata": {
        "id": "K26_cP5mS41A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}